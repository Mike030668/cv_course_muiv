{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aca20f8",
   "metadata": {},
   "source": [
    "# Семинарский ноутбук по компьютерному зрению (Module 4)\n",
    "\n",
    "В этом задании вам предстоит применить и усовершенствовать методы компьютерного зрения, рассмотренные в лекции, а также\n",
    "провести ряд экспериментов. Ноутбук представляет собой **шаблон**, в котором нужно дописать код и ответы в обозначенных местах.\n",
    "\n",
    "## Задачи\n",
    "\n",
    "1. **HOG + SVM**: подберите параметры HOG и оцените влияние на точность классификатора SVM.\n",
    "2. **NMS**: сравните результаты работы Non‑Maximum Suppression при порогах IoU 0.3, 0.5 и 0.7.\n",
    "3. **PR‑кривые**: постройте точность–полноту для двух детекторов с разными распределениями скор.\n",
    "4. **Нормированная кросс‑корреляция (NCC)**: попробуйте другие шаблоны и оцените чувствительность к шуму.\n",
    "5. **Морфологические операции**: замените пример на своё изображение и протестируйте разные ядра.\n",
    "6. **Аугментации и нейронная сеть**: добавьте аугментации и сравните валидационную точность модели.\n",
    "7. **Сегментация**: протестируйте предобученную модель сегментации на своих изображениях, измените палитру и прозрачность наложения.\n",
    "\n",
    "> **Подсказки**: В каждой секции приведены краткие объяснения, формулы и заготовки функций. Заполняйте `TODO`‑блоки и выполняйте эксперименты. Отмечайте свои наблюдения в Markdown‑ячейках после выполнения кода.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2775ec",
   "metadata": {},
   "source": [
    "## 1. HOG + SVM\n",
    "\n",
    "Гистограммы ориентированных градиентов (HOG) позволяют извлекать **локальные особенности формы**. Изображение разбивается на ячейки, в каждой ячейке строится гистограмма направлений градиента, а затем гистограммы нормируются по блокам. Векторы HOG‑признаков подаются на вход линейному SVM, который обучается различать классы.\n",
    " \n",
    "Формулы градиентных компонент:  \n",
    "$$\n",
    "g_x = I(x+1, y) - I(x-1, y), \\quad\n",
    "g_y = I(x, y+1) - I(x, y-1),\n",
    "$$  \n",
    "$$\n",
    "m = \\sqrt{g_x^2 + g_y^2}, \\quad\n",
    "\\theta = \\operatorname{atan2}(g_y, g_x).\n",
    "$$\n",
    "\n",
    "Нормировка $L_2$-гистограммы в блоке:  \n",
    "$$\n",
    "\\hat{h} = \\frac{h}{\\sqrt{\\lVert h \\rVert_2^2 + \\varepsilon^2}}.\n",
    "$$\n",
    "\n",
    "### Что нужно сделать\n",
    "\n",
    "- Реализуйте функцию для вычисления HOG‑признаков (можно использовать `skimage.feature.hog` или написать самостоятельно).\n",
    "- Используйте датасет `digits` из `sklearn` для распознавания рукописных цифр.\n",
    "- Проведите **перебор различных размеров ячеек и блоков** (например, \\(2\\times2\\), \\(4\\times4\\), \\(8\\times8\\)) и оцените влияние на точность линейного SVM.\n",
    "- Отобразите несколько примеров гистограмм или визуализацию HOG‑признаков.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf37b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: HOG + SVM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Вы можете использовать skimage для вычисления HOG, либо написать свою функцию\n",
    "# from skimage.feature import hog\n",
    "\n",
    "# 1) Загрузка данных\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# 2) Разделите данные на тренировочную и тестовую выборки\n",
    "# X_train, X_test, y_train, y_test = ... (используйте train_test_split)\n",
    "\n",
    "# 3) Напишите функцию compute_hog(img, cell_size=(4,4), block_size=(2,2), bins=9)\n",
    "#    которая возвращает вектор признаков HOG для изображения\n",
    "# def compute_hog(image, cell_size=(4,4), block_size=(2,2), bins=9):\n",
    "#     # TODO: реализуйте вычисление градиентов и гистограмм\n",
    "#     pass\n",
    "\n",
    "# 4) Переберите несколько параметров cell_size и block_size\n",
    "#    и обучите линейный SVM (например, sklearn.svm.LinearSVC) на HOG‑признаках\n",
    "#    Для каждого набора параметров вычислите точность классификации\n",
    "\n",
    "# 5) Визуализируйте несколько HOG‑признаков (например, используя skimage.feature.hog(return_hog_image=True))\n",
    "#    и отметьте, как меняется представление при разных размерах ячеек\n",
    "\n",
    "# В этой ячейке оставьте только код. Ваши выводы запишите в следующей Markdown‑ячейке.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9246f2",
   "metadata": {},
   "source": [
    "## 2. Non‑Maximum Suppression (NMS)\n",
    "\n",
    "В задачах детектирования объектов алгоритм *Non‑Maximum Suppression* (NMS) позволяет убрать дублирующиеся\n",
    "детекции, оставляя только наилучшие. Он основан на метрике **Intersection over Union (IoU)**:\n",
    "\n",
    "$$\n",
    "\\operatorname{IoU}(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "\n",
    "### Что нужно сделать\n",
    "\n",
    "* Реализуйте функцию для вычисления IoU между двумя прямоугольниками.\n",
    "* Напишите функцию NMS, которая принимает список прямоугольников с оценками (score) и порог IoU, и возвращает индексы\n",
    "  выбранных прямоугольников.\n",
    "* Создайте несколько примерных прямоугольников и оценок и протестируйте NMS при порогах **0.3, 0.5, 0.7**.\n",
    "* Проанализируйте, как меняется количество оставшихся детекций при изменении порога.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fedbe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: NMS implementation\n",
    "import numpy as np\n",
    "\n",
    "# 1) Функция вычисления площади пересечения / объединения\n",
    "# def iou(box_a, box_b):\n",
    "#     # box = [x1, y1, x2, y2]\n",
    "#     # TODO: вычислите пересечение и объединение\n",
    "#     return 0.0\n",
    "\n",
    "# 2) Реализация NMS\n",
    "# def non_max_suppression(boxes, scores, iou_threshold=0.5):\n",
    "#     # TODO: сортировка по score и отбор боксов\n",
    "#     # используйте iou(...) для вычисления перекрытия\n",
    "#     return []  # верните индексы выбранных прямоугольников\n",
    "\n",
    "# Пример входных данных\n",
    "boxes = np.array([\n",
    "    [50, 50, 150, 150],\n",
    "    [60, 60, 140, 140],\n",
    "    [200, 200, 300, 300],\n",
    "    [220, 220, 320, 320]\n",
    "])\n",
    "scores = np.array([0.9, 0.8, 0.95, 0.7])\n",
    "\n",
    "# 3) Протестируйте NMS для разных порогов IoU\n",
    "# for thr in [0.3, 0.5, 0.7]:\n",
    "#     keep_idx = non_max_suppression(boxes, scores, iou_threshold=thr)\n",
    "#     print(f\"Порог IoU = {thr}: выбрано {len(keep_idx)} прямоугольников\")\n",
    "\n",
    "# Выводы запишите в следующей ячейке.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8055a",
   "metadata": {},
   "source": [
    "## 3. Построение PR‑кривых\n",
    "\n",
    "Точность–полнота (precision–recall) — важная метрика для оценки детекторов, особенно при несбалансированных данных.\n",
    "Вычисляется на основе истинно положительных (TP), ложноположительных (FP) и ложоотрицательных (FN) предсказаний:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}, \\quad \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}.\n",
    "$$\n",
    "\n",
    "### Что нужно сделать\n",
    "\n",
    "* Создайте две выборки оценок (scores) для положительных и отрицательных объектов с разными средними значениями — \n",
    "  имитируя «сильный» и «слабый» детектор.\n",
    "* Используйте `sklearn.metrics.precision_recall_curve` для построения точностно‑полнотной кривой для каждого детектора.\n",
    "* Нарисуйте кривые на одном графике, подписав легенду. Вычислите и сравните площади под кривыми (AUC).\n",
    "* Сделайте выводы о том, какой детектор лучше и почему.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ccb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PR curves\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# 1) Сгенерируйте синтетические оценки для положительных и отрицательных примеров\n",
    "# Например: positive_scores = np.random.normal(loc=0.7, scale=0.1, size=100)\n",
    "#           negative_scores = np.random.normal(loc=0.3, scale=0.1, size=100)\n",
    "\n",
    "# 2) Создайте массивы y_true и y_scores для двух детекторов (сильный и слабый)\n",
    "# y_true = ... # 1 для положительных, 0 для отрицательных\n",
    "# y_scores_detector1 = ...\n",
    "# y_scores_detector2 = ...\n",
    "\n",
    "# 3) Постройте precision‑recall кривые с помощью precision_recall_curve(...)\n",
    "# precision1, recall1, _ = precision_recall_curve(y_true, y_scores_detector1)\n",
    "# precision2, recall2, _ = precision_recall_curve(y_true, y_scores_detector2)\n",
    "\n",
    "# 4) Вычислите площадь под кривой (AUC)\n",
    "# auc1 = auc(recall1, precision1)\n",
    "# auc2 = auc(recall2, precision2)\n",
    "\n",
    "# 5) Визуализируйте кривые\n",
    "# plt.figure()\n",
    "# plt.plot(recall1, precision1, label=f'Detector 1 (AUC={auc1:.2f})')\n",
    "# plt.plot(recall2, precision2, label=f'Detector 2 (AUC={auc2:.2f})')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.legend()\n",
    "# plt.title('Precision–Recall curves')\n",
    "# plt.show()\n",
    "\n",
    "# Выводы сформулируйте ниже.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179001f",
   "metadata": {},
   "source": [
    "## 4. Нормированная кросс‑корреляция (NCC)\n",
    "\n",
    "Нормированная кросс‑корреляция используется для поиска шаблонов в изображениях. Функция `cv2.matchTemplate` с флагом\n",
    "`cv2.TM_CCOEFF_NORMED` возвращает карту совпадений, где максимальное значение соответствует наилучшему совпадению.\n",
    "\n",
    "### Что нужно сделать\n",
    "\n",
    "* Загрузите изображение и выберите из него несколько разных шаблонов (разных размеров или участков).\n",
    "* Реализуйте функцию, которая возвращает координаты максимальной корреляции и значение корреляции.\n",
    "* Добавьте в исходное изображение шум различного уровня (например, `np.random.normal`), повторите поиск шаблонов и\n",
    "  сравните значения корреляций.\n",
    "* Сделайте выводы о влиянии шума и выборе шаблона на результаты.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Template matching with NCC\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Загрузите изображение, например, используйте cv2.imread('path/to/your/image.png')\n",
    "# img = cv2.imread(..., cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# 2) Вырежьте шаблон (patch) из изображения и попробуйте несколько разных шаблонов\n",
    "# template = img[y0:y1, x0:x1]\n",
    "\n",
    "# 3) Функция для нахождения максимума NCC\n",
    "# def find_best_match(img, template):\n",
    "#     result = cv2.matchTemplate(img, template, method=cv2.TM_CCOEFF_NORMED)\n",
    "#     min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "#     return max_val, max_loc\n",
    "\n",
    "# 4) Попробуйте добавить шум в изображение\n",
    "# noisy = img + np.random.normal(0, 10, img.shape)\n",
    "# noisy = np.clip(noisy, 0, 255).astype('uint8')\n",
    "\n",
    "# 5) Для каждого шаблона сравните max_val на чистом и шумном изображениях и сделайте вывод\n",
    "\n",
    "# Выводы запишите в следующей Markdown‑ячейке.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6423881",
   "metadata": {},
   "source": [
    "## 5. Морфологические операции\n",
    "\n",
    "Морфологические операции позволяют изменять структуру бинарных изображений. **Диляция** расширяет объекты, **эрозия** —\n",
    "сужает, **открытие** и **закрытие** объединяют последовательность эрозии и диляции для удаления шума или заполнения дыр.\n",
    "\n",
    "### Что нужно сделать\n",
    "\n",
    "* Загрузите собственное изображение и превратите его в бинарное (черно‑белое) с помощью `cv2.threshold` или `cv2.adaptiveThreshold`.\n",
    "* Создайте структурные элементы разных форм и размеров: квадратный, прямоугольный, круглый (`cv2.getStructuringElement`).\n",
    "* Примените к бинарному изображению различные морфологические операции: `cv2.dilate`, `cv2.erode`, `cv2.morphologyEx(..., cv2.MORPH_OPEN)`,\n",
    "  `cv2.morphologyEx(..., cv2.MORPH_CLOSE)` и др.\n",
    "* Сравните результаты и выберите наиболее подходящую комбинацию для улучшения изображения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bb8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Morphological operations\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Загрузите своё изображение в оттенках серого\n",
    "# img = cv2.imread('path/to/your/image.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# 2) Получите бинарное изображение (используйте threshold или adaptiveThreshold)\n",
    "# _, binary = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# 3) Создайте структурные элементы разных форм\n",
    "# kernel_rect = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "# kernel_circle = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "\n",
    "# 4) Примените морфологические операции\n",
    "# dilated = cv2.dilate(binary, kernel_rect)\n",
    "# eroded = cv2.erode(binary, kernel_rect)\n",
    "# opened = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_circle)\n",
    "# closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel_circle)\n",
    "\n",
    "# 5) Визуализируйте результаты для сравнения\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "# axes[0].imshow(binary, cmap='gray'); axes[0].set_title('Исходное')\n",
    "# axes[1].imshow(dilated, cmap='gray'); axes[1].set_title('Диляция')\n",
    "# axes[2].imshow(eroded, cmap='gray'); axes[2].set_title('Эрозия')\n",
    "# for ax in axes: ax.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# Добавьте аналогично визуализацию открытия и закрытия\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06971279",
   "metadata": {},
   "source": [
    "## 6. Аугментации и простая нейронная сеть\n",
    "\n",
    "Полноценные свёрточные нейронные сети (CNN) требуют фреймворков вроде PyTorch или TensorFlow, но для экспериментов в этом\n",
    "ноутбуке можно использовать `scikit‑learn` и многослойный перцептрон (`MLPClassifier`).\n",
    "\n",
    "### Что нужно сделать\n",
    "\n",
    "* Загрузите датасет `digits` из `sklearn.datasets`.\n",
    "* Создайте базовую модель MLP и обучите её на исходных данных, измерив точность на валидационной выборке.\n",
    "* Реализуйте простые аугментации изображений — например, поворот на ±10° и добавление случайного шума — и увеличьте обучающую\n",
    "  выборку.\n",
    "* Обучите модель на расширенных данных и сравните точность с базовой моделью. Какой прирост даёт аугментация?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MLP with augmentations\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# 1) Загрузите данные\n",
    "# X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# 2) Разделите на обучение и валидацию\n",
    "# X_train, X_val, y_train, y_val = train_test_split(...)\n",
    "\n",
    "# 3) Обучите базовую модель (без аугментации)\n",
    "# base_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200)\n",
    "# base_model.fit(X_train, y_train)\n",
    "# y_pred_base = base_model.predict(X_val)\n",
    "# base_acc = accuracy_score(y_val, y_pred_base)\n",
    "# print('Baseline accuracy:', base_acc)\n",
    "\n",
    "# 4) Реализуйте аугментации (поворот, шум) для изображений X_train\n",
    "# def augment_image(img):\n",
    "#     # Поворт на случайный угол в пределах [-10°, 10°]\n",
    "#     aug = rotate(img.reshape(8,8), angle=np.random.uniform(-10, 10), reshape=False)\n",
    "#     # Добавьте шум\n",
    "#     noise = np.random.normal(0, 0.1, aug.shape)\n",
    "#     aug = np.clip(aug + noise, 0, 16)\n",
    "#     return aug.flatten()\n",
    "\n",
    "# X_train_aug = np.array([augment_image(img) for img in X_train])\n",
    "# y_train_aug = y_train.copy()\n",
    "\n",
    "# 5) Обучите модель на аугментированных данных и сравните точность\n",
    "# model_aug = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200)\n",
    "# model_aug.fit(X_train_aug, y_train_aug)\n",
    "# y_pred_aug = model_aug.predict(X_val)\n",
    "# aug_acc = accuracy_score(y_val, y_pred_aug)\n",
    "# print('Augmented accuracy:', aug_acc)\n",
    "\n",
    "# Запишите выводы ниже.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e618e",
   "metadata": {},
   "source": [
    "## 7. Сегментация изображений\n",
    "\n",
    "Для выделения объектов на изображении можно использовать различные алгоритмы:\n",
    "\n",
    "* **GrabCut** — интерактивный метод сегментации, встроенный в OpenCV. Требует инициализации прямоугольником,\n",
    "  внутри которого предполагается находиться интересующий объект.\n",
    "* **DeepLabv3** — свёрточная нейросеть для семантической сегментации (доступна в `torchvision.models.segmentation`). Для\n",
    "  корректной работы в среде может понадобиться подключение к интернету для загрузки предобученных весов.\n",
    "\n",
    "### Что нужно сделать\n",
    "\n",
    "* Выберите изображение, на котором нужно выделить объект (например, фотографию с простым фоном).\n",
    "* Воспользуйтесь `cv2.grabCut` для сегментации и получите бинарную маску объекта. Поиграйте с положением и размером\n",
    "  прямоугольной инициализации.\n",
    "* (Дополнительно) Если доступен PyTorch, загрузите предобученную модель `deeplabv3_resnet50` и получите маску классов. Наложите\n",
    "  её на исходное изображение, используя свой выбор цветов и коэффициента прозрачности.\n",
    "* Измените палитру сегментации (цвет маски) и уровень прозрачности \\(\u0007lpha\\), чтобы получить наилучший визуальный результат.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61925711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Image segmentation\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Вариант 1: GrabCut\n",
    "# 1) Загрузите изображение\n",
    "# img = cv2.imread('path/to/your/photo.jpg')\n",
    "\n",
    "# 2) Задайте прямоугольник (x, y, width, height), внутри которого находится объект\n",
    "# rect = (x, y, w, h)\n",
    "\n",
    "# 3) Инициализируйте маски и модели\n",
    "# mask = np.zeros(img.shape[:2], np.uint8)\n",
    "# bgModel = np.zeros((1,65), np.float64)\n",
    "# fgModel = np.zeros((1,65), np.float64)\n",
    "\n",
    "# 4) Запустите GrabCut\n",
    "# cv2.grabCut(img, mask, rect, bgModel, fgModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# 5) Превратите mask в бинарную и получите сегментированное изображение\n",
    "# seg_mask = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "# segmented = img.copy()\n",
    "# segmented[seg_mask == 0] = 0\n",
    "\n",
    "# 6) Наложите цветную маску с прозрачностью\n",
    "# color = np.array([0, 255, 0])  # зелёный цвет\n",
    "# overlay = img.copy()\n",
    "# overlay[seg_mask == 1] = (0.5 * overlay[seg_mask == 1] + 0.5 * color).astype('uint8')\n",
    "\n",
    "# 7) Покажите исходное изображение, маску и результат\n",
    "# plt.figure(figsize=(12,4))\n",
    "# plt.subplot(1,3,1); plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)); plt.title('Original')\n",
    "# plt.subplot(1,3,2); plt.imshow(seg_mask, cmap='gray'); plt.title('Mask')\n",
    "# plt.subplot(1,3,3); plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)); plt.title('Overlay')\n",
    "# for ax in plt.gcf().axes: ax.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# Вариант 2 (дополнительно): используя torchvision.models.segmentation.deeplabv3_resnet50\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "# # ... загрузите модель, подготовьте изображение, получите mask ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f311dd4",
   "metadata": {},
   "source": [
    "## 8. Выводы и обсуждение\n",
    "\n",
    "После выполнения всех заданий сформулируйте краткие выводы:\n",
    "\n",
    "* Какие параметры HOG оказались наиболее эффективными для вашего датасета?\n",
    "* Как порог IoU влияет на количество выбранных детекций в NMS?\n",
    "* Как различается качество двух детекторов на PR‑кривой? Как интерпретировать площадь под кривой?\n",
    "* Насколько устойчивы результаты NCC к шуму и выбору шаблона?\n",
    "* Какие морфологические операции и ядра лучше подходят для вашего изображения и почему?\n",
    "* Дала ли аугментация прирост точности МLP? Какие трансформации наиболее полезны?\n",
    "* Какой метод сегментации вы выбрали и какие настройки цветовой маски использовали?\n",
    "\n",
    "Запишите свои наблюдения и выводы.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
