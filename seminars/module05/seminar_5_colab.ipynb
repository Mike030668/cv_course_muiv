{"cells":[{"cell_type":"markdown","id":"fe4b53a4","metadata":{"id":"fe4b53a4"},"source":["# Установка и базовые импорты"]},{"cell_type":"code","execution_count":1,"id":"e1480a8a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e1480a8a","executionInfo":{"status":"ok","timestamp":1762947604142,"user_tz":-180,"elapsed":28102,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"156cffa4-c90e-4720-948d-1cd2ce07cefd"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://cdn.pixabay.com/video/2024/05/08/211188_large.mp4\n","To: /content/test_video.mp4\n","100%|██████████| 45.7M/45.7M [00:00<00:00, 58.5MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["[PosixPath('/content/test_video.mp4')]"]},"metadata":{},"execution_count":1}],"source":["import gdown, os, sys, glob, math, time\n","from pathlib import Path\n","import cv2, subprocess, shlex, numpy as np, os\n","from pathlib import Path\n","import numpy as np\n","import torch\n","from torchvision import transforms\n","from torchvision.models.video import r3d_18, R3D_18_Weights\n","\n","# Using a different, generally compatible HTML5 video\n","#html_video = \"https://www.learningcontainer.com/wp-content/uploads/2020/05/sample-mp4-file.mp4\"\n","html_video = 'https://cdn.pixabay.com/video/2024/05/08/211188_large.mp4'\n","#html_video = \"https://cdn.pixabay.com/video/2016/10/11/5871-186389068_large.mp4\"\n","path_video = '/content/test_video.mp4'\n","\n","\n"," # ← укажите папку с видео\n","video_path = Path(path_video)\n","video_path.parent.mkdir(parents=True, exist_ok=True)\n","\n","# Remove existing video if it clashes with the new download\n","if video_path.exists():\n","    os.remove(video_path)\n","\n","try:\n","    gdown.download(html_video, str(video_path), quiet=False)\n","except Exception:\n","    pass\n","\n","\n","# небольшие хелперы\n","#VIDEOS_DIR = video_path  # initial value\n","OUT_DIR = Path(\"out\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n","\n","def list_videos(vdir: Path):\n","    vids = sorted([p for ext in (\"*.mp4\",\"*.avi\",\"*.mov\",\"*.mkv\") for p in vdir.glob(ext)])\n","    if not vids:\n","        raise FileNotFoundError(f\"В {vdir} не найдено видеофайлов.\")\n","    return vids\n","\n","# If path_video is a file that exists, use it directly; otherwise treat as directory\n","if video_path.exists() and video_path.is_file():\n","    videos = [video_path]\n","else:\n","    VIDEOS_DIR = video_path if video_path.is_dir() else video_path.parent\n","    videos = list_videos(VIDEOS_DIR)\n","\n","videos[:3]"]},{"cell_type":"code","execution_count":2,"id":"62e94fb2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":932,"output_embedded_package_id":"1-CZiRp2LxSsnrrmhzoYB7lYzEjAMiljA"},"id":"62e94fb2","executionInfo":{"status":"ok","timestamp":1762947621959,"user_tz":-180,"elapsed":9343,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"9fd9d28f-b640-46a3-98e2-bb65dbec4a37"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import base64\n","from IPython.display import HTML, display\n","def display_video_from_path(video_path: str) -> HTML:\n","    \"\"\"\n","    Reads a video file, encodes it as base64, and embeds it within an IPython.display.HTML object for display.\n","    This method is often more robust for displaying local video files in Colab.\n","\n","    Args:\n","        video_path (str): The path to the video file.\n","\n","    Returns:\n","        IPython.display.HTML: An HTML object containing the embedded video.\n","    \"\"\"\n","    with open(video_path, 'rb') as video_file:\n","        encoded_video = base64.b64encode(video_file.read()).decode('utf-8')\n","\n","    video_html = f\"\"\"\n","    <video width='512' controls autoplay>\n","        <source src='data:video/mp4;base64,{encoded_video}' type='video/mp4'>\n","        Your browser does not support the video tag.\n","    </video>\n","    \"\"\"\n","    return HTML(video_html)\n","\n","src_path = str(videos[0])\n","display(display_video_from_path(src_path))"]},{"cell_type":"markdown","id":"6b6c9cd9","metadata":{"id":"6b6c9cd9"},"source":["# Работа с видео в CV (чтение, запись, кадры, FPS)\n","\n","Ключевые вещи: открыть поток, прочитать/обработать кадры, сохранить результат."]},{"cell_type":"code","execution_count":3,"id":"0e54905d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0e54905d","executionInfo":{"status":"ok","timestamp":1762947767744,"user_tz":-180,"elapsed":126497,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"7f061d30-a656-4335-bfd7-d725eb4a9e23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Готово: /content/out/basic_h264_repaired.mp4\n"]}],"source":["import cv2, subprocess, shlex, numpy as np, os\n","from pathlib import Path\n","\n","# Вспомогательная функция: открыть ffmpeg-пайп под H.264\n","def open_h264_writer(dst_path: str, w: int, h: int, fps: float, crf: int = 20, preset: str = \"veryfast\"):\n","    \"\"\"\n","    Создаёт процесс ffmpeg, который примет сырые кадры (BGR24) через stdin\n","    и запишет MP4 c H.264 (yuv420p, +faststart), совместимый с HTML5.\n","    Возвращает subprocess.Popen с открытым stdin.\n","    \"\"\"\n","    Path(dst_path).parent.mkdir(parents=True, exist_ok=True)\n","    cmd = (\n","        f'ffmpeg -y -f rawvideo -pix_fmt bgr24 -s {w}x{h} -r {fps} -i - '\n","        f'-an -c:v libx264 -preset {preset} -crf {crf} -pix_fmt yuv420p '\n","        f'-movflags +faststart \"{dst_path}\"'\n","    )\n","    return subprocess.Popen(shlex.split(cmd), stdin=subprocess.PIPE)\n","\n","# Куда сохранить результирующее видео (совместимое с HTML5)\n","dst_path = \"/content/out/basic_h264_repaired.mp4\"\n","Path(dst_path).parent.mkdir(parents=True, exist_ok=True)\n","\n","# Открываем источник видео (должна быть определена переменная src_path)\n","cap = cv2.VideoCapture(src_path)\n","assert cap.isOpened(), f\"Не открыть {src_path}\"\n","\n","# Извлекаем параметры входного видео\n","# fps: частота кадров, w/h: ширина/высота кадра (используем их для корректного пайпа в ffmpeg)\n","fps = cap.get(cv2.CAP_PROP_FPS) or 25  # на некоторых файлах CAP_PROP_FPS может вернуть 0 → подставляем 25\n","w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","# Используем open_h264_writer\n","# Готовим команду ffmpeg и создаём процесс\n","proc = open_h264_writer(dst_path, w, h, fps)\n","\n","# Читаем видео из OpenCV по кадрам и передаём их в stdin ffmpeg\n","# Важно: frame.tobytes() → последовательность байт в формате bgr24, как указано в командной строке\n","while True:\n","    ok, frame = cap.read()\n","    if not ok:\n","        break\n","    proc.stdin.write(frame.tobytes())\n","\n","# Корректно закрываем ресурсы\n","cap.release()           # освобождаем видеодекодер OpenCV\n","proc.stdin.close()      # закрываем stdin, чтобы ffmpeg понял, что поток завершён\n","proc.wait()             # дожидаемся завершения ffmpeg и записи файла на диск\n","\n","print(\"Готово:\", dst_path)\n"]},{"cell_type":"code","execution_count":4,"id":"ba328557","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":932,"output_embedded_package_id":"1-K-3N9SE4kYZQaYY4bsmroKzCSt_GbWo"},"id":"ba328557","executionInfo":{"status":"ok","timestamp":1762947775385,"user_tz":-180,"elapsed":7645,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"da92341c-d628-4e54-df2e-051d79064b30"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["display(display_video_from_path(dst_path))"]},{"cell_type":"markdown","id":"a8badfcf","metadata":{"id":"a8badfcf"},"source":["# Оптический поток: визуализация и применения\n","\n","**Оптический поток** — это векторное поле смещений между соседними кадрами: для каждого «наблюдаемого» элемента сцены оцениваем, куда он сдвинулся за $\\Delta t$.\n","\n","## Базовые предпосылки\n","\n","* **Неизменность яркости (brightness constancy):**\n","\n","  $$\n","  I(x,y,t) \\approx I(x+u,,y+v,,t+\\Delta t),\n","  $$\n","\n","  где $(u,v)$ — смещение пикселя.\n","\n","* **Малость движения и гладкость поля** (для локальной аппроксимации и устойчивости).\n","\n","Из первой предпосылки, после линеаризации (Тейлор) получаем **основное уравнение оптического потока**:\n","\n","$$\n","I_x,u + I_y,v + I_t \\approx 0,\n","$$\n","\n","где $I_x, I_y$ — пространственные производные, $I_t$ — временная.\n","\n","---\n","\n","## А) Разреженный Lucas–Kanade (LK)\n","\n","Идея: в небольшой окрестности $W$ вокруг точки считаем смещение **одним и тем же**, решаем задачу наименьших квадратов:\n","\n","$$\n","\\min_{u,v}\\ \\sum_{(x,y)\\in W} \\big(I_x(x,y),u + I_y(x,y),v + I_t(x,y)\\big)^2.\n","$$\n","\n","Нормальные уравнения:\n","\n","Тогда\n","$$\n","\\underbrace{\n","\\begin{pmatrix}\n","\\sum I_x^2 & \\sum I_x I_y\\\\[4pt]\n","\\sum I_x I_y & \\sum I_y^2\n","\\end{pmatrix}\n","}_{\\mathbf{A}}\n","\\begin{pmatrix} u\\\\ v \\end{pmatrix}\n","=\n","-\\underbrace{\n","\\begin{pmatrix}\n","\\sum I_x I_t\\\\[4pt]\n","\\sum I_y I_t\n","\\end{pmatrix}\n","}_{\\mathbf{b}} \\, .\n","$$\n","\n","Если \\(\\det(\\mathbf{A})\\) не близок к нулю, то\n","$$\n","\\begin{pmatrix} u\\\\ v \\end{pmatrix}\n","= -\\,\\mathbf{A}^{-1}\\mathbf{b}.\n","$$\n","Чтобы система была устойчива, нужны «текстурные» точки, где есть градиент и по $x$, и по $y$. Поэтому выбираем точки по **Ши–Томаси** (good features to track).\n","\n","**На практике:**\n","\n","* Окно $W$ (напр. $21\\times21$), пирамиды (image pyramids) для больших смещений.\n","* Выдаёт смещения только в выбранных **точках** (разреженно).\n","* Визуализация: стрелки/траектории по «углам» (Shi–Tomasi) — компактно и быстро.\n","\n","**Когда выбирать LK:** нужно быстро отследить опорные точки/ключевые объекты; важна скорость и простота.\n","\n","---\n","\n","## Б) Плотный Farnebäck\n","\n","Идея: аппроксимируем локальные окрестности **квадратичными полиномами** яркости и подбираем смещение между кадрами, которое согласует эти аппроксимации.\n","\n","Локальная модель (первый кадр):\n","$$\n","I_1(\\mathbf{x}) \\approx \\mathbf{x}^\\top \\mathbf{A},\\mathbf{x} + \\mathbf{b}^\\top \\mathbf{x} + c,\n","$$\n","во втором кадре $I_2(\\mathbf{x}) \\approx I_1(\\mathbf{x}-\\mathbf{d})$; подбираем $\\mathbf{d}=(u,v)$, минимизируя несоответствие полиномиальных приближений. Далее уточняем на пирамиде (coarse-to-fine) и сглаживаем поле.\n","\n","**Свойства:**\n","\n","* Даёт **вектор** для **каждого пикселя** (плотный поток).\n","* Хорошо работает на «мягких» текстурах за счёт полиномиальной модели.\n","* Параметры: масштаб пирамиды, число уровней, размер окна, число итераций.\n","\n","**Визуализация (HSV):**\n","\n","* Hue (оттенок) — направление вектора $\\angle (u,v)$,\n","* Value (яркость) — величина $,\\sqrt{u^2+v^2}$,\n","* Saturation — фиксируем 255.\n","  Так получаем «карту движения» с интуитивной цветовой кодировкой.\n","\n","**Когда выбирать Farnebäck:** нужен **полный** поток для сегментации движения, оценки фона/фора, построения масок и последующего трекинга.\n","\n","---\n","\n","## Типичные применения\n","\n","* **Сегментация движения**: порог по величине $|{\\bf d}|$ → бинарная маска «где движется».\n","* **Инициализация/поддержка трекинга**: маска движения + bbox (мы показали на практике: поток → маска → контуры → рамки).\n","* **Стабилизация**: оценка глобального смещения/деформации (выравнивание кадров).\n","* **Аналитика жестов/активности**: суммарные векторы по ROI, гистограммы направлений и т. п.\n","\n","---\n","\n","## Что помнить на практике\n","\n","* **Освещение/блики** нарушают «неизменность яркости» → возможны артефакты.\n","* **Большие смещения** → используем **пирамиду** и/или даунскейл + итерации.\n","* **Текстурность** важна для LK: выбираем точки (Shi–Томаси) там, где есть градиент в обеих координатах.\n","* Для «шумных» сцен — постобработка: сглаживание, морфология, NMS по bbox (как в ноутбуке).\n","\n","---\n","\n","## Куда «крутить» ручки\n","\n","* **LK:** размер окна $W$, число уровней пирамиды, критерий сходимости.\n","  Больше окно/уровней → устойчивее к шуму и крупным смещениям, но медленнее.\n","* **Farnebäck:** `pyrScale`, `numLevels`, `winSize`, `numIters`, `polyN`, `polySigma`.\n","  Увеличение `winSize`/`numIters` — более гладкий и устойчивый поток; `polyN/polySigma` — компромисс «детали↔шум».\n","\n","---\n","\n","### Коротко: когда что использовать\n","\n","* **Нужно быстро и наглядно понять траектории ключевых точек** → LK (разреженно).\n","* **Нужна карта движения и производные маски/боксы** → Farnebäck (плотно, с HSV-визуализацией).\n"]},{"cell_type":"code","execution_count":5,"id":"3e332d80","metadata":{"id":"3e332d80","executionInfo":{"status":"ok","timestamp":1762947786230,"user_tz":-180,"elapsed":127,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[],"source":["# ВАЖНО: как и раньше, пишем результат через ffmpeg-пайп в H.264 (yuv420p, +faststart),\n","# чтобы ролики гарантированно проигрывались встроенным HTML5-плеером в ноутбуке.\n","\n","import cv2, numpy as np, subprocess, shlex\n","from pathlib import Path\n","\n","src_path = str(videos[0])         # берём то же входное видео, что и раньше\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# ---------- Общие параметры видео ----------\n","cap = cv2.VideoCapture(src_path)\n","assert cap.isOpened(), f\"Не открыть видео: {src_path}\"\n","w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))   # ширина кадра\n","h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # высота кадра\n","fps = cap.get(cv2.CAP_PROP_FPS) or 25          # частота кадров (если 0 — подставляем 25)\n","\n","\n"]},{"cell_type":"markdown","id":"fa93b8c5","metadata":{"id":"fa93b8c5"},"source":["## А) Lucas–Kanade (разреженный): трекинг «углов» между кадрами\n","\n","Делаем визуал явным:\n","\n","- для LK: стрелки, толщина, фильтр по длине вектора, цвет по направлению;\n","\n","- для Farneback: цветная карта (HSV) уже есть — просто сохраним и покажем.\n","\n","**Lucas–Kanade: заметная визуализация (стрелки + цвет)**"]},{"cell_type":"code","execution_count":6,"id":"ce2ef28b","metadata":{"id":"ce2ef28b","outputId":"2c27ccb3-c88e-4f66-a8a3-988de7f0fd55","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762948009310,"user_tz":-180,"elapsed":209946,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Готово: out/optical_flow_LK_side_by_side_h264.mp4\n"]}],"source":["# Разреженный оптический поток (Lucas–Kanade) — две панели:\n","# слева оригинальный кадр, справа — стрелки/точки на чёрном фоне.\n","# Цвет стрелок кодирует направление (угол) через HSV→BGR.\n","# Параметры min_len / arrow_scale / max_points_to_draw управляют читаемостью.\n","\n","import cv2, numpy as np, subprocess, shlex\n","from pathlib import Path\n","\n","src_path = str(videos[0])\n","cap = cv2.VideoCapture(src_path); assert cap.isOpened(), f\"Не открыть: {src_path}\"\n","w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","fps = cap.get(cv2.CAP_PROP_FPS) or 25\n","\n","\n","# ===== Настройки визуализации =====\n","min_len = 1.5             # игнорировать «дрожь» короче порога (пиксели)\n","arrow_scale = 2.5         # во сколько раз удлинять стрелки для наглядности\n","tip_len = 0.35            # относительная длина наконечника стрелки (0..1)\n","thickness = 2             # толщина линий\n","max_points_to_draw = 300  # максимум стрелок на кадр (для читаемости)\n","\n","# Выход: ширина в 2 раза больше (две панели рядом)\n","dst_path = str(OUT_DIR / \"optical_flow_LK_side_by_side_h264.mp4\")\n","proc = open_h264_writer(dst_path, w*2, h, fps)\n","\n","# ===== Инициализация LK =====\n","ok, prev = cap.read(); assert ok, \"Пустое видео?\"\n","prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n","\n","# Стартовые «углы» (Shi–Tomasi) — те точки, которые LK будет трекать\n","p0 = cv2.goodFeaturesToTrack(\n","    prev_gray,\n","    maxCorners=800,      # больше — гуще поле\n","    qualityLevel=0.01,\n","    minDistance=7,\n","    blockSize=7\n",")\n","\n","lk_params = dict(\n","    winSize=(21, 21),    # окно для локальной аппроксимации\n","    maxLevel=3,          # пирамидальные уровни (для крупных смещений)\n","    criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01)\n",")\n","\n","while True:\n","    ok, frame = cap.read()\n","    if not ok:\n","        break\n","\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","    # Если точки «иссякли», переинициализируем (не рвём поток)\n","    if p0 is None or len(p0) == 0:\n","        p0 = cv2.goodFeaturesToTrack(gray, maxCorners=800, qualityLevel=0.01, minDistance=7, blockSize=7)\n","        # Покажем пустую правую панель (стрелок пока нет)\n","        right = np.zeros((h, w, 3), dtype=np.uint8)\n","        stacked = np.hstack([frame, right])\n","        proc.stdin.write(stacked.tobytes())\n","        prev_gray = gray\n","        continue\n","\n","    # LK: сопоставляем «углы» между prev_gray и текущим gray\n","    p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, gray, p0, None, **lk_params)\n","\n","    # Правая панель — «чистый» чёрный фон под стрелки\n","    right = np.zeros((h, w, 3), dtype=np.uint8)\n","\n","    if p1 is not None and st is not None:\n","        good_new = p1[st == 1]   # новые позиции тех точек, что удалось сопоставить\n","        good_old = p0[st == 1]   # соответствующие старые позиции\n","\n","        if good_new.size > 0:\n","            # Векторы смещения и углы\n","            vec = good_new - good_old\n","            dx, dy = vec[:, 0], vec[:, 1]\n","            mag = np.sqrt(dx*dx + dy*dy)          # длина вектора\n","            ang = np.arctan2(dy, dx) + np.pi      # угол [0..2π)\n","\n","            # Отсеиваем «дрожь»\n","            keep = mag > min_len\n","            if np.any(keep):\n","                gn = good_new[keep]\n","                go = good_old[keep]\n","                ang_keep = ang[keep]\n","                mag_keep = mag[keep]\n","\n","                # Прореживание, чтобы не «забить» кадр\n","                if len(gn) > max_points_to_draw:\n","                    idx = np.random.choice(len(gn), size=max_points_to_draw, replace=False)\n","                    gn, go, ang_keep, mag_keep = gn[idx], go[idx], ang_keep[idx], mag_keep[idx]\n","\n","                # Цвет стрелок по направлению (Hue), насыщенность/яркость — максимальные\n","                hue = (ang_keep / (2*np.pi) * 179).astype(np.uint8).reshape(-1, 1)\n","                hsv = np.zeros((len(hue), 1, 3), dtype=np.uint8)\n","                hsv[..., 0] = hue\n","                hsv[..., 1] = 255\n","                hsv[..., 2] = 255\n","                colors = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR).reshape(-1, 3).tolist()\n","\n","                # Рисуем стрелки и конечные точки на правой панели (оригинал — слева без изменений)\n","                for (x1, y1), (x0, y0), color in zip(gn, go, colors):\n","                    # Увеличим длину для наглядности\n","                    vx = (x1 - x0) * arrow_scale\n","                    vy = (y1 - y0) * arrow_scale\n","                    x0i, y0i = int(x0), int(y0)\n","                    x1i, y1i = int(x0 + vx), int(y0 + vy)\n","\n","                    cv2.arrowedLine(right, (x0i, y0i), (x1i, y1i),\n","                                    color, thickness=thickness, tipLength=tip_len)\n","                    cv2.circle(right, (x1i, y1i), 3, color, -1)\n","\n","    # Склеиваем две панели: [оригинал | стрелки]\n","    stacked = np.hstack([frame, right])\n","    proc.stdin.write(stacked.tobytes())\n","\n","    # Обновляем «предыдущий кадр» и точки — продолжаем треки\n","    prev_gray = gray\n","    p0 = None if (p1 is None or st is None) else good_new.reshape(-1, 1, 2)\n","\n","cap.release()\n","proc.stdin.close()\n","proc.wait()\n","print(\"Готово:\", dst_path)\n"]},{"cell_type":"code","execution_count":7,"id":"20df0b57","metadata":{"id":"20df0b57","outputId":"95f1ba4e-d65e-4ed7-cc23-6ae4526fc29f","colab":{"base_uri":"https://localhost:8080/","height":476,"output_embedded_package_id":"1_9VhZCpZtS7fzWnAqNtwk8VLBAW4TVlI"},"executionInfo":{"status":"ok","timestamp":1762948029483,"user_tz":-180,"elapsed":9950,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["display(display_video_from_path(dst_path))"]},{"cell_type":"markdown","id":"9d8eae0b","metadata":{"id":"9d8eae0b"},"source":["## B) Плотный оптический поток\n","визуализация потока на чёрном фоне\n","\n","Делает быстро и робастно:\n","- Если доступен GPU (OpenCV с CUDA), использует cv2.cuda (TV-L1 или Farneback).\n","- Иначе — CPU Farneback на уменьшенной копии кадра, затем апскейл в исходный размер.\n","- Запись сразу в H.264 (yuv420p, +faststart), чтобы HTML5-плеер в ноутбуке играл без сюрпризов."]},{"cell_type":"code","execution_count":22,"id":"74540093","metadata":{"id":"74540093","outputId":"949de088-6b64-4517-bbce-bba7f8102905","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762953816682,"user_tz":-180,"elapsed":520747,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Готово: out/flow_mask_orig_bbox_nms_h264.mp4\n"]}],"source":["# Плотный поток + МАСКА + ОРИГИНАЛ С BBOX + NMS\n","# ┌──────────────┬─────────────────────┬────────────── ┐\n","# │  FLOW (HSV)  │  MOTION MASK (bin)  │ ORIGINAL+BBOX │\n","# └──────────────┴─────────────────────┴────────────── ┘\n","# - формируем маску движения по перцентилю;\n","# - чистим морфологией;\n","# - собираем контуры → боксы;\n","# - считаем \"оценку\" бокса = средняя скорость в mag внутри рамки;\n","# - применяем NMS (порог по IoU), чтобы убрать дубликаты/перекрытия;\n","# - рисуем более \"жирные\" боксы.\n","\n","import cv2, numpy as np, subprocess, shlex\n","from pathlib import Path\n","\n","src_path = str(videos[0])\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","dst_path = str(OUT_DIR / \"flow_mask_orig_bbox_nms_h264.mp4\")\n","\n","# --- тюнинг производительности ---\n","MAX_SIDE = 720\n","SKIP     = 0\n","\n","# --- параметры маски/боксов/NMS ---\n","PERC       = 95           # порог по перцентилю скорости\n","K_OPEN     = 3            # морф. открытие (шум)\n","K_DILATE   = 5            # дилатация (слияние)\n","MIN_AREA   = 800          # минимальная площадь бокса\n","IOU_THR    = 0.5          # порог для NMS (чем выше, тем агрессивнее)\n","DRAW_COLOR = (0, 255, 255)\n","THICK      = 4            # толще рамки\n","\n","\n","def iou_xywh(a, b):\n","    \"\"\"IoU для боксов в формате (x, y, w, h).\"\"\"\n","    ax1, ay1, aw, ah = a; ax2, ay2 = ax1 + aw, ay1 + ah\n","    bx1, by1, bw, bh = b; bx2, by2 = bx1 + bw, by1 + bh\n","    inter_x1, inter_y1 = max(ax1, bx1), max(ay1, by1)\n","    inter_x2, inter_y2 = min(ax2, bx2), min(ay2, by2)\n","    inter_w, inter_h = max(0, inter_x2 - inter_x1), max(0, inter_y2 - inter_y1)\n","    inter = inter_w * inter_h\n","    union = aw * ah + bw * bh - inter\n","    return inter / union if union > 0 else 0.0\n","\n","def nms_xywh(boxes, scores, iou_thr=0.5):\n","    \"\"\"Простая NMS: сортируем по score убыв., отбираем, гасим перекрывающиеся > iou_thr.\"\"\"\n","    idxs = np.argsort(scores)[::-1]\n","    keep = []\n","    suppressed = np.zeros(len(idxs), dtype=bool)\n","    for i in range(len(idxs)):\n","        if suppressed[i]:\n","            continue\n","        keep.append(idxs[i])\n","        for j in range(i + 1, len(idxs)):\n","            if suppressed[j]:\n","                continue\n","            if iou_xywh(boxes[idxs[i]], boxes[idxs[j]]) > iou_thr:\n","                suppressed[j] = True\n","    return [boxes[k] for k in keep], [scores[k] for k in keep]\n","\n","# --- исходное видео ---\n","cap = cv2.VideoCapture(src_path); assert cap.isOpened(), f\"Не открыть: {src_path}\"\n","W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","FPS = cap.get(cv2.CAP_PROP_FPS) or 25\n","\n","# вычислительный даунскейл\n","short, long = (H, W) if H <= W else (W, H)\n","scale = min(1.0, MAX_SIDE / short)\n","proc_W = max(2, int(W * scale)) // 2 * 2\n","proc_H = max(2, int(H * scale)) // 2 * 2\n","sx = W / proc_W\n","sy = H / proc_H\n","\n","# полотно из 3 панелей по ширине\n","out_w, out_h = W * 3, H\n","proc = open_h264_writer(dst_path, out_w, out_h, FPS)\n","\n","# первый кадр\n","ok, prev = cap.read(); assert ok, \"Пустое видео?\"\n","for _ in range(SKIP): cap.grab()\n","prev_small = cv2.resize(prev, (proc_W, proc_H), interpolation=cv2.INTER_AREA) if scale < 1.0 else prev\n","prev_gray_small = cv2.cvtColor(prev_small, cv2.COLOR_BGR2GRAY)\n","\n","kernel_open   = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (K_OPEN, K_OPEN))\n","kernel_dilate = cv2.getStructuringElement(cv2.MORPH_RECT,   (K_DILATE, K_DILATE))\n","\n","while True:\n","    ok, frame = cap.read()\n","    if not ok:\n","        break\n","    for _ in range(SKIP): cap.grab()\n","\n","    # расчёт плотного потока (на даунскейле для скорости)\n","    curr_small = cv2.resize(frame, (proc_W, proc_H), interpolation=cv2.INTER_AREA) if scale < 1.0 else frame\n","    curr_gray_small = cv2.cvtColor(curr_small, cv2.COLOR_BGR2GRAY)\n","    flow_small = cv2.calcOpticalFlowFarneback(prev_gray_small, curr_gray_small,\n","                                              None, 0.5, 3, 15, 3, 5, 1.2, 0)\n","\n","    # апскейл вектора к оригиналу\n","    fx = cv2.resize(flow_small[..., 0], (W, H), interpolation=cv2.INTER_LINEAR) * sx\n","    fy = cv2.resize(flow_small[..., 1], (W, H), interpolation=cv2.INTER_LINEAR) * sy\n","\n","    # панель 1: цветная карта потока (HSV→BGR)\n","    mag, ang = cv2.cartToPolar(fx, fy)\n","    hsv = np.zeros((H, W, 3), dtype=np.uint8)\n","    hsv[..., 0] = (ang * 180 / np.pi / 2).astype(np.uint8)\n","    hsv[..., 1] = 255\n","    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n","    flow_vis = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","\n","    # панель 2: маска движения\n","    thr = np.percentile(mag, PERC) if np.any(mag) else 0.0\n","    mask = (mag > thr).astype(np.uint8) * 255\n","    if K_OPEN > 1:\n","        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_open, iterations=1)\n","    if K_DILATE > 1:\n","        mask = cv2.dilate(mask, kernel_dilate, iterations=1)\n","    mask_vis = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n","\n","    # панель 3: оригинал + боксы (после NMS)\n","    orig_with_bbox = frame.copy()\n","\n","    # контуры → боксы → скор (средняя скорость в маг)\n","    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    boxes, scores = [], []\n","    for c in contours:\n","        x, y, w, h = cv2.boundingRect(c)\n","        if w * h < MIN_AREA:\n","            continue\n","        # оценка: средняя скорость внутри бокса — устойчивее площади/максимума\n","        mean_speed = float(mag[y:y+h, x:x+w].mean())\n","        boxes.append((x, y, w, h))\n","        scores.append(mean_speed)\n","\n","    # NMS по IoU, чтобы убрать дубли/накладки\n","    if boxes:\n","        boxes_nms, scores_nms = nms_xywh(boxes, scores, iou_thr=IOU_THR)\n","        for (x, y, w, h) in boxes_nms:\n","            cv2.rectangle(orig_with_bbox, (x, y), (x + w, y + h), DRAW_COLOR, THICK)\n","\n","    # склейка панелей\n","    canvas = np.hstack([flow_vis, mask_vis, orig_with_bbox])\n","    proc.stdin.write(canvas.tobytes())\n","\n","    prev_gray_small = curr_gray_small\n","\n","cap.release()\n","proc.stdin.close()\n","proc.wait()\n","print(\"Готово:\", dst_path)\n"]},{"cell_type":"code","execution_count":23,"id":"97f6f409","metadata":{"id":"97f6f409","outputId":"b1c77b5b-c05e-4689-d9d1-95befd92a5d8","colab":{"base_uri":"https://localhost:8080/","height":325,"output_embedded_package_id":"1Tt8LFcPisXsgOlunik42_QFjsmsDL4mX"},"executionInfo":{"status":"ok","timestamp":1762953833782,"user_tz":-180,"elapsed":17103,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["display(display_video_from_path(dst_path))"]},{"cell_type":"markdown","id":"e7676029","metadata":{"id":"e7676029"},"source":["# Калман-фильтр для сглаживания 2D-позиции объекта (центр bbox) + YOLO (person)\n","\n","**Идея.** Отслеживаем центр рамки детектора как 2D-точку и считаем, что объект движется с почти постоянной скоростью. Детектор даёт шумные измерения позиции — Калман сглаживает и умеет предсказывать, когда измерений нет.\n","\n","## Модель состояния (constant velocity)\n","\n","Состояние:\n","$$\n","\\mathbf{x}_k =\n","\\begin{bmatrix}\n","x_k\\\\\n","y_k\\\\\n","v^x_k\\\\\n","v^y_k\n","\\end{bmatrix},\\quad \\mathbf{u}_k \\equiv 0.\n","$$\n","\n","Динамика за шаг $\\Delta t$:\n","$$\n","\\mathbf{x}_k = \\mathbf{F}\\,\\mathbf{x}_{k-1} + \\mathbf{w}_k,\\qquad\n","\\mathbf{F}=\n","\\begin{bmatrix}\n","1&0&\\Delta t&0\\\\\n","0&1&0&\\Delta t\\\\\n","0&0&1&0\\\\\n","0&0&0&1\n","\\end{bmatrix},\\quad\n","\\mathbf{w}_k \\sim \\mathcal{N}(\\mathbf{0},\\,\\mathbf{Q}).\n","$$\n","\n","Обычно берут $\\mathbf{Q}=q\\,\\mathbf{I}_4$ (чем больше $q$, тем быстрее реакция на манёвры).\n","\n","## Модель измерения (YOLO → центр bbox)\n","\n","Детектор возвращает центр рамки $(z^x_k, z^y_k)$:\n","$$\n","\\mathbf{z}_k =\n","\\begin{bmatrix}\n","z^x_k\\\\\n","z^y_k\n","\\end{bmatrix}\n","=\n","\\mathbf{H}\\,\\mathbf{x}_k + \\mathbf{v}_k,\\qquad\n","\\mathbf{H}=\n","\\begin{bmatrix}\n","1&0&0&0\\\\\n","0&1&0&0\n","\\end{bmatrix},\\quad\n","\\mathbf{v}_k \\sim \\mathcal{N}(\\mathbf{0},\\,\\mathbf{R}).\n","$$\n","\n","Часто $\\mathbf{R}=\\sigma^2\\,\\mathbf{I}_2$ (чем больше $\\sigma^2$, тем меньше доверие детектору).\n","\n","## Рекурсии Калмана\n","\n","**Прогноз:**\n","$$\n","\\hat{\\mathbf{x}}^-_k = \\mathbf{F}\\,\\hat{\\mathbf{x}}_{k-1},\\qquad\n","\\mathbf{P}^-_k = \\mathbf{F}\\,\\mathbf{P}_{k-1}\\,\\mathbf{F}^\\top + \\mathbf{Q}.\n","$$\n","\n","**Коррекция (при наличии $\\mathbf{z}_k$):**\n","$$\n","\\mathbf{S}_k = \\mathbf{H}\\,\\mathbf{P}^-_k\\,\\mathbf{H}^\\top + \\mathbf{R},\\quad\n","\\mathbf{K}_k = \\mathbf{P}^-_k\\,\\mathbf{H}^\\top\\,\\mathbf{S}_k^{-1},\n","$$\n","$$\n","\\hat{\\mathbf{x}}_k = \\hat{\\mathbf{x}}^-_к + \\mathbf{K}_k\\!\\left(\\mathbf{z}_k-\\mathbf{H}\\hat{\\mathbf{x}}^-_k\\right),\\quad\n","\\mathbf{P}_k = (\\mathbf{I}-\\mathbf{K}_k\\mathbf{H})\\,\\mathbf{P}^-_k.\n","$$\n","\n","Если детектор молчит — делаем только прогноз.\n","\n","## Как тюнить шумы\n","\n","- Больше $q$ в $\\mathbf{Q}$ → быстрее подстраиваемся к резким поворотам/ускорениям.\n","- Больше $\\sigma^2$ в $\\mathbf{R}$ → сильнее сглаживание «дрожащих» измерений.\n","- Нестрогий FPS → обновляй $\\Delta t$ как фактический интервал между кадрами.\n","\n","\n"]},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfmrbX_u7YrF","executionInfo":{"status":"ok","timestamp":1762948696774,"user_tz":-180,"elapsed":8602,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"26578247-25e5-4d80-a4ba-1168e86a71b5"},"id":"DfmrbX_u7YrF","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.227-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n","Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.10.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n","Downloading ultralytics-8.3.227-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.3.227 ultralytics-thop-2.0.18\n"]}]},{"cell_type":"code","execution_count":12,"id":"dd8f729e","metadata":{"id":"dd8f729e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762948701593,"user_tz":-180,"elapsed":291,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"eab52cf7-62c5-4290-9e09-1e372aad2e7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["# Практика:\n","#   - Детектируем людей (COCO class 0) самой лёгкой моделью \"yolov8n.pt\".\n","#   - Берём самый крупный bbox в кадре (устойчивее при множестве людей).\n","#   - Визуализация:\n","#       * зелёная точка — «сырое» измерение детектора,\n","#       * синяя точка — оценка Калмана (сглаженная/предсказанная).\n","#   - Сохранение: пишем сразу в H.264 (yuv420p, +faststart) через ffmpeg-пайп — корректно играет в ноутбуке.\n","\n","import cv2, numpy as np, subprocess, shlex\n","from pathlib import Path\n","from ultralytics import YOLO\n","\n","\n","class Kalman2D:\n","    \"\"\"\n","    Линейный Калман под модель «равномерного движения»:\n","        state = [x, y, vx, vy]^T\n","        measurement = [x, y]^T\n","    dt — шаг по времени (1/FPS), process_var — дисперсия процесса (насколько доверяем модели),\n","    meas_var — дисперсия измерения (насколько шумны наблюдения детектора).\n","    \"\"\"\n","    def __init__(self, dt: float = 1.0, process_var: float = 1.0, meas_var: float = 25.0):\n","        self.kf = cv2.KalmanFilter(4, 2, 0)\n","\n","        # Матрица перехода (constant velocity model)\n","        self.kf.transitionMatrix = np.array([\n","            [1, 0, dt, 0 ],\n","            [0, 1, 0 , dt],\n","            [0, 0, 1 , 0 ],\n","            [0, 0, 0 , 1 ],\n","        ], np.float32)\n","\n","        # Матрица наблюдения: видим только позицию\n","        self.kf.measurementMatrix = np.array([\n","            [1, 0, 0, 0],\n","            [0, 1, 0, 0],\n","        ], np.float32)\n","\n","        # Ковариация процессного шума (Q): чем больше — тем быстрее «догоняем» резкие манёвры\n","        q = float(process_var)\n","        self.kf.processNoiseCov = np.array([\n","            [q, 0, 0, 0],\n","            [0, q, 0, 0],\n","            [0, 0, q, 0],\n","            [0, 0, 0, q],\n","        ], np.float32)\n","\n","        # Ковариация шума измерений (R): чем больше — тем меньше доверяем детектору\n","        r = float(meas_var)\n","        self.kf.measurementNoiseCov = np.array([\n","            [r, 0],\n","            [0, r],\n","        ], np.float32)\n","\n","        # Начальная апостериорная ковариация и состояние\n","        self.kf.errorCovPost = np.eye(4, dtype=np.float32)\n","        self.kf.statePost = np.zeros((4, 1), np.float32)\n","\n","        self.initialized = False  # выставим true при первом «валидном» измерении\n","\n","    def _init_from_measurement(self, xy: np.ndarray):\n","        \"\"\"Инициализация состояния первой валидной позицией (нулевая скорость).\"\"\"\n","        x, y = float(xy[0]), float(xy[1])\n","        self.kf.statePost[:] = np.array([[x], [y], [0.0], [0.0]], np.float32)\n","        self.initialized = True\n","\n","    def predict(self) -> np.ndarray:\n","        \"\"\"Прогноз позиции [x, y] без коррекции (когда нет наблюдения).\"\"\"\n","        pred = self.kf.predict()\n","        return pred[:2].reshape(-1)  # (x, y)\n","\n","    def correct(self, xy: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Коррекция по измерению [x, y]. Если фильтр не инициализирован — инициализируем.\n","        Возвращает оценку позиции после коррекции.\n","        \"\"\"\n","        xy = np.asarray(xy, dtype=np.float32).reshape(2, 1)\n","        if not self.initialized:\n","            self._init_from_measurement(xy.ravel())\n","            return xy.ravel()\n","        est = self.kf.correct(xy)\n","        return est[:2].reshape(-1)\n","\n","def bbox_to_center_xyxy(b: np.ndarray) -> tuple[float, float]:\n","    \"\"\"Перевод bbox формата [x1, y1, x2, y2] → центр (x, y).\"\"\"\n","    x1, y1, x2, y2 = b\n","    return (float((x1 + x2) * 0.5), float((y1 + y2) * 0.5))\n"]},{"cell_type":"code","execution_count":13,"id":"bda9d947","metadata":{"id":"bda9d947","outputId":"32def551-f9a4-49e0-8023-7073bde1c93c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762948896778,"user_tz":-180,"elapsed":185492,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 241.3MB/s 0.0s\n","Калман-демо записано в: out/kalman_demo_h264.mp4\n"]}],"source":["# --- вход/выход ---\n","src_path = str(videos[0])\n","cap = cv2.VideoCapture(src_path); assert cap.isOpened(), f\"Не открыть: {src_path}\"\n","w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","fps = cap.get(cv2.CAP_PROP_FPS) or 25\n","\n","dst_path = str(OUT_DIR / \"kalman_demo_h264.mp4\")\n","proc = open_h264_writer(dst_path, w, h, fps, crf=20, preset=\"veryfast\")\n","\n","# --- детектор и фильтр ---\n","yolo = YOLO(\"yolov8n.pt\")                         # лёгкая COCO-модель\n","kf = Kalman2D(dt=1.0/float(fps), process_var=1.0, meas_var=50.0)  # базовые шумы: под ситуацию можно тюнить\n","\n","while True:\n","    ok, frame = cap.read()\n","    if not ok:\n","        break\n","\n","    # Быстрый детект по кадру (без накопления результатов в RAM)\n","    res = yolo(frame, stream=False, verbose=False)[0]\n","\n","    # Собираем bbox людей (class 0 = person), оставляем уверенные\n","    bboxes = []\n","    if res.boxes is not None and len(res.boxes) > 0:\n","        xyxy = res.boxes.xyxy.cpu().numpy()\n","        cls   = res.boxes.cls.cpu().numpy().astype(int)\n","        conf  = res.boxes.conf.cpu().numpy()\n","        for b, c, p in zip(xyxy, cls, conf):\n","            if c == 0 and p > 0.3:       # человек с conf > 0.3\n","                bboxes.append(b)\n","\n","    meas_xy = None\n","    if bboxes:\n","        # Берём самый большой bbox — устойчивее от случайных мелких срабатываний\n","        areas = [(b[2]-b[0])*(b[3]-b[1]) for b in bboxes]\n","        b = bboxes[int(np.argmax(areas))]\n","        x1, y1, x2, y2 = map(int, b)\n","        # Рисуем «сырое» измерение: зелёную рамку и зелёную точку в центре\n","        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 5)\n","        meas_xy = bbox_to_center_xyxy(b)\n","        #cv2.circle(frame, (int(meas_xy[0]), int(meas_xy[1])), 5, (0, 255, 0), -1)\n","        cv2.circle(frame, (int(meas_xy[0]), int(meas_xy[1])), 9, (0, 255, 0), -1)\n","        cv2.circle(frame, (int(meas_xy[0]), int(meas_xy[1])), 11, (0, 100, 0), 2)\n","\n","    # Шаг предсказания\n","    pred_xy = kf.predict()\n","\n","    # Шаг коррекции (если есть наблюдение)\n","    if meas_xy is not None:\n","        est_xy = kf.correct(meas_xy)\n","    else:\n","        est_xy = pred_xy  # нет измерения — остаёмся на предсказании\n","\n","    # Визуализация оцененной позиции Калмана (синяя точка) и «хвостик» скорости\n","    ex, ey = int(est_xy[0]), int(est_xy[1])\n","    cv2.circle(frame, (ex, ey), 11, (255, 0, 0), -1)\n","    cv2.circle(frame, (ex, ey), 13, (60, 0, 0), 2)\n","    cv2.putText(frame, \"Kalman (blue) vs measurement (green)\", (20, h - 20),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 4)    # тень\n","    cv2.putText(frame, \"Kalman (blue) vs measurement (green)\", (20, h - 20),\n","                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)  # текст\n","\n","\n","    # Пишем кадр в ffmpeg\n","    proc.stdin.write(frame.tobytes())\n","\n","cap.release()\n","proc.stdin.close()\n","proc.wait()\n","print(\"Калман-демо записано в:\", dst_path)\n"]},{"cell_type":"code","execution_count":14,"id":"ca46f4cd","metadata":{"id":"ca46f4cd","outputId":"cdc47107-63ab-4798-b0a0-031894d80e59","colab":{"base_uri":"https://localhost:8080/","height":932,"output_embedded_package_id":"1OKGIz3add1lggVvESpJVTTvjX4oYYvmp"},"executionInfo":{"status":"ok","timestamp":1762948915283,"user_tz":-180,"elapsed":8261,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["display(display_video_from_path(dst_path))"]},{"cell_type":"markdown","id":"8be1aacb","metadata":{"id":"8be1aacb"},"source":["# Классификация действия: 3D-CNN r3d_18 (Kinetics-400)\n","\n","**Задача.** По короткому клипу (последовательности кадров) определить **класс действия**.\n","\n","## Почему 3D-свёртки?\n","Обычная 2D-CNN видит только пространственные паттерны внутри одного кадра.\n","3D-CNN (как r3d_18) накладывает **свёртку 3×k×k** по времени и пространству → учит\n","совместные **пространственно-временные признаки** (движения и их форму).\n","\n","## Что делает r3d_18\n","- Архитектура: **ResNet-18**, где все свёртки заменены на **3D-свёртки**.\n","- Вход: тензор размера **(C, T, H, W)**, обычно `C=3`, `T≈8–32` кадров.\n","- Выход: логиты на **Kinetics-400** (400 классов: “dribbling basketball”, “playing guitar”, …).\n","\n","## Мини-инференс для демонстрации\n","1) Читаем видео покадрово и поддерживаем скользящее окно из `T=16` кадров.  \n","2) Каждые `S` кадров (шаг) прогоняем окно через `r3d_18 (Kinetics-400)` → получаем вероятности классов.  \n","3) На кадрах между двумя инференсами держим последний предсказанный `top-1` (как «стабильную подпись»).  \n","4) Сохраняем аннотированный ролик (overlay: класс + вероятность) и простую временную диаграмму класса по кадрам.\n","\n","> Для скорости перед подачей в сеть уменьшаем размер кадров (например, короткая сторона → 256) и даём модели `16×256×256`.\n","> На практике для надёжной оценки берут **несколько клипов** из разных мест ролика,\n","> иногда с разными кропами/масштабами, и усредняют предсказания.\n"]},{"cell_type":"code","execution_count":15,"id":"bf65374b","metadata":{"id":"bf65374b","outputId":"e037e96b-ae67-4c9c-b69b-8ad67c4b9f9b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762949109213,"user_tz":-180,"elapsed":162259,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 127M/127M [00:02<00:00, 53.1MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["clip shape (T,H,W,3): (16, 3840, 2160, 3)\n","\n","Top-5 классов:\n","1. faceplanting — 0.230\n","2. digging — 0.194\n","3. feeding birds — 0.136\n","4. crossing river — 0.071\n","5. slacklining — 0.066\n","Аннотированное видео сохранено: out/r3d18_action_overlay_h264.mp4\n"]}],"source":["# Классификация действия (r3d_18, Kinetics-400) + проигрывание видео с оверлеем top-5\n","# — выбираем 16 кадров равномерно по ролику\n","# — прогоняем через r3d_18 -> top-5 классов\n","# — сохраняем исходное видео c «табличкой» top-5 поверх (стабильно для HTML5)\n","\n","import cv2, torch, numpy as np, subprocess, shlex\n","from pathlib import Path\n","from torchvision.models.video import r3d_18, R3D_18_Weights\n","\n","# ----- входы/выходы -----\n","VIDEO_PATH = str(videos[0])  # иначе возьмём первый в списке\n","\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","ANN_PATH = str(OUT_DIR / \"r3d18_action_overlay_h264.mp4\")  # аннотированный ролик\n","\n","# ----- модель/препроцесс -----\n","DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","WEIGHTS  = R3D_18_Weights.DEFAULT\n","MODEL    = r3d_18(weights=WEIGHTS).eval().to(DEVICE)\n","PREPROC  = WEIGHTS.transforms()\n","CLASSES  = WEIGHTS.meta[\"categories\"]\n","\n","# ----- утилиты -----\n","def sample_clip_uniform(path: str, num_frames: int = 16):\n","    \"\"\"Равномерно выбираем num_frames кадров из ролика. Возвращает (T,H,W,3) RGB uint8 и индексы кадров.\"\"\"\n","    cap = cv2.VideoCapture(path); assert cap.isOpened(), f\"Не открыть: {path}\"\n","    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    idxs = np.linspace(0, max(0, n-1), num_frames).astype(int)\n","    frames, picked = [], []\n","    i = 0\n","    ok = True\n","    while ok and i <= idxs[-1]:\n","        ok, bgr = cap.read()\n","        if not ok: break\n","        if i in idxs:\n","            frames.append(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))\n","            picked.append(i)\n","        i += 1\n","    cap.release()\n","    # дублируем последний, если видео слишком короткое\n","    while len(frames) < num_frames and len(frames)>0:\n","        frames.append(frames[-1]); picked.append(picked[-1])\n","    return np.stack(frames, axis=0), np.array(picked, dtype=int)\n","\n","def to_model_tensor(clip_np: np.ndarray) -> torch.Tensor:\n","    \"\"\"\n","    (T,H,W,3) uint8 -> PREPROC -> (1,3,T,H,W) float32 на DEVICE.\n","    У PREPROC оси могут быть (C,T,H,W) или (T,C,H,W) — унифицируем.\n","    \"\"\"\n","    vid = torch.from_numpy(clip_np).permute(0,3,1,2).contiguous()  # (T,3,H,W)\n","    out = PREPROC(vid)  # обычно (3,T,H,W)\n","    if out.ndim != 4:\n","        raise RuntimeError(f\"Ожидался 4D тензор после PREPROC, пришло: {tuple(out.shape)}\")\n","    if out.shape[0] == 3:              # (C,T,H,W)\n","        clip = out.unsqueeze(0)        # -> (1,3,T,H,W)\n","    else:                              # (T,C,H,W)\n","        clip = out.permute(1,0,2,3).unsqueeze(0)\n","    return clip.to(DEVICE)\n","\n","\n","def draw_top5_panel(frame_bgr, top5, origin=(20, 20),\n","                    width=800, row_h=56,              # шире панель и выше строки\n","                    font_scale=2.0, thickness=3,      # крупный шрифт\n","                    alpha=0.55):                      # прозрачность подложки\n","    \"\"\"\n","    Крупная полупрозрачная панель с Top-5.\n","    - width: ширина панели\n","    - row_h: высота строки (межстрочный интервал)\n","    - font_scale, thickness: размер/толщина шрифта\n","    - alpha: доля подложки (0..1)\n","    \"\"\"\n","    x0, y0 = origin\n","    rows = len(top5) + 1\n","    pad  = 14\n","    h = pad + rows * row_h\n","\n","    overlay = frame_bgr.copy()\n","    # фон-подложка\n","    cv2.rectangle(overlay, (x0, y0), (x0 + width, y0 + h), (0, 0, 0), -1)\n","    cv2.addWeighted(overlay, alpha, frame_bgr, 1 - alpha, 0, frame_bgr)\n","\n","    # заголовок\n","    cv2.putText(frame_bgr, \"R3D-18 / Kinetics-400 (Top-5)\",\n","                (x0 + pad, y0 + int(row_h * 0.7)),\n","                cv2.FONT_HERSHEY_SIMPLEX, font_scale, (240, 240, 240),\n","                thickness, cv2.LINE_AA)\n","\n","    # строки top-5\n","    for k, (lab, p) in enumerate(top5, start=1):\n","        text = f\"{k}. {lab}  ({p:.2f})\"\n","        y = y0 + int(row_h * (k + 0.7))\n","        cv2.putText(frame_bgr, text, (x0 + pad, y),\n","                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255),\n","                    thickness, cv2.LINE_AA)\n","\n","# ----- 1) Клип -> предсказание -----\n","NUM_FRAMES = 16\n","clip_np, picked_idx = sample_clip_uniform(VIDEO_PATH, NUM_FRAMES)  # (T,H,W,3), индексы кадров\n","print(\"clip shape (T,H,W,3):\", clip_np.shape)\n","\n","with torch.no_grad():\n","    clip_tensor = to_model_tensor(clip_np)          # (1,3,T,H,W) на DEVICE\n","    logits = MODEL(clip_tensor)                     # (1,400)\n","    probs  = torch.softmax(logits, dim=1)[0].cpu().numpy()\n","\n","# top-5 списком [(label, prob), ...]\n","top5_idx = probs.argsort()[-5:][::-1]\n","TOP5 = [(CLASSES[i], float(probs[i])) for i in top5_idx]\n","\n","print(\"\\nTop-5 классов:\")\n","for k,(lab,p) in enumerate(TOP5, start=1):\n","    print(f\"{k}. {lab} — {p:.3f}\")\n","\n","# ----- 2) Проигрываем исходное видео с оверлеем top-5 -----\n","cap = cv2.VideoCapture(VIDEO_PATH); assert cap.isOpened()\n","fps = cap.get(cv2.CAP_PROP_FPS) or 25\n","w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","writer = open_h264_writer(ANN_PATH, w, h, fps, crf=20, preset=\"veryfast\")\n","\n","while True:\n","    ok, frame = cap.read()\n","    if not ok: break\n","    draw_top5_panel(frame, TOP5, origin=(20, 20),\n","                width=900, row_h=60, font_scale=2.2, thickness=4, alpha=0.6)\n","    writer.stdin.write(frame.tobytes())\n","\n","cap.release(); writer.stdin.close(); writer.wait()\n","print(\"Аннотированное видео сохранено:\", ANN_PATH)\n"]},{"cell_type":"code","execution_count":16,"id":"8ad91fd7","metadata":{"id":"8ad91fd7","outputId":"f74931b4-7571-4bf8-aff5-dc371a578909","colab":{"base_uri":"https://localhost:8080/","height":932,"output_embedded_package_id":"1IvecEDqUjiVdy2JJVSor4PfpQ5L0D1jR"},"executionInfo":{"status":"ok","timestamp":1762949117821,"user_tz":-180,"elapsed":8612,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["display(display_video_from_path(ANN_PATH))"]},{"cell_type":"markdown","id":"b80db956","metadata":{"id":"b80db956"},"source":["# Комбинация детекции (YOLO) + трекинг (встроенный ByteTrack)\n","\n","В учебных целях используем Ultralytics YOLO с готовым трекером bytetrack (простая и практичная связка)."]},{"cell_type":"code","execution_count":17,"id":"53649666","metadata":{"id":"53649666","outputId":"d53a1aa7-bcec-45a0-ff11-bf7e8fd4ce5c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762949481682,"user_tz":-180,"elapsed":192186,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n","Using Python 3.12.12 environment at: /usr\n","Resolved 2 packages in 130ms\n","Prepared 1 package in 29ms\n","Installed 1 package in 5ms\n"," + lap==0.5.12\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.8s\n","WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":17}],"source":["# Детекция (YOLO) + трекинг (ByteTrack) с немедленной записью результата в H.264,\n","# чтобы видео гарантированно воспроизводилось в HTML5-плеере ноутбука.\n","\n","from ultralytics import YOLO\n","import cv2, subprocess, shlex, numpy as np\n","from pathlib import Path\n","\n","# 1) Загружаем лёгкую предобученную модель детекции (COCO)\n","yolo = YOLO(\"yolov8n.pt\")\n","\n","# 2) Источник видео и путь, куда писать итоговый ролик (совместимый с браузером)\n","src_path = str(videos[0])\n","dst_path = str(Path(OUT_DIR) / \"yolo_bytetrack\" / \"yolo_track_run\" / \"tracked_direct_h264.mp4\")\n","Path(dst_path).parent.mkdir(parents=True, exist_ok=True)\n","\n","# 3) Получаем параметры входного видео для корректной конфигурации ffmpeg-пайпа\n","cap = cv2.VideoCapture(src_path)\n","fps = cap.get(cv2.CAP_PROP_FPS) or 25    # частота кадров (если 0 — подставим 25)\n","w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","# 4) Готовим ffmpeg-процесс, который будет принимать сырые кадры (BGR) по stdin и кодировать их в H.264\n","#    -f rawvideo      : вход — сырые кадры без контейнера\n","#    -pix_fmt bgr24   : формат пикселей OpenCV (3 канала по 8 бит)\n","#    -s {w}x{h}       : размер кадра, ffmpeg должен знать точные размеры RAW-потока\n","#    -r {fps}         : частота кадров\n","#    -i -             : читать из stdin\n","#    -an              : без аудио (для простоты учебного примера)\n","#    -c:v libx264     : кодек H.264 (совместим с HTML5)\n","#    -preset veryfast : ускоренная кодировка (хорошо для демо)\n","#    -crf 20          : целевое визуальное качество (чем выше CRF — тем сильнее сжатие)\n","#    -pix_fmt yuv420p : требуемый браузером формат\n","#    -movflags +faststart : перенести moov-атом в начало (быстрый старт стриминга в браузере)\n","cmd = (\n","    f'ffmpeg -y -f rawvideo -pix_fmt bgr24 -s {w}x{h} -r {fps} -i - '\n","    f'-an -c:v libx264 -preset veryfast -crf 20 -pix_fmt yuv420p '\n","    f'-movflags +faststart \"{dst_path}\"'\n",")\n","proc = subprocess.Popen(shlex.split(cmd), stdin=subprocess.PIPE)\n","\n","# 5) Запускаем трекинг в потоковом режиме:\n","#    stream=True — выдаёт генератор результатов, не копит их в память (важно для длинных роликов)\n","#    tracker=\"bytetrack.yaml\" — вшитая конфигурация ByteTrack для ассоциации детекций в треки\n","#    conf / iou — пороги детекции и NMS\n","for r in yolo.track(source=src_path, tracker=\"bytetrack.yaml\",\n","                    conf=0.25, iou=0.5, stream=True, verbose=False):\n","    # r.plot() рисует на кадре рамки, метки классов, ID треков — получаем готовый кадр для записи\n","    frame = r.plot()\n","    # Пишем байты кадра (BGR) напрямую в stdin ffmpeg — он кодирует и пишет MP4 на диск\n","    proc.stdin.write(frame.tobytes())\n","\n","# 6) Аккуратно закрываем ресурсы: видеопоток и stdin ffmpeg (чтобы он завершил файл корректно)\n","cap.release()\n","proc.stdin.close()\n","proc.wait()\n","\n"]},{"cell_type":"code","execution_count":18,"id":"e0ad12ca","metadata":{"id":"e0ad12ca","outputId":"911850e9-b49c-4d19-af9d-1ce2ae3cb24e","colab":{"base_uri":"https://localhost:8080/","height":932,"output_embedded_package_id":"1AcytggoZZWJUxw6RC85uQq392_8VQN_z"},"executionInfo":{"status":"ok","timestamp":1762949549174,"user_tz":-180,"elapsed":7488,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["display(display_video_from_path(dst_path))"]},{"cell_type":"markdown","id":"cd794b3e","metadata":{"id":"cd794b3e"},"source":["## Метрики по трекингу: средняя скорость и время в зоне\n","\n","**Дано.** Для объекта с ID \\(i\\) известны центры его боксов по кадрам:\n","$$\n","c_t^{(i)} = \\big(x_t^{(i)},\\, y_t^{(i)}\\big), \\quad t=1,\\dots,T_i,\n","$$\n","частота кадров — \\(f\\) FPS.\n","\n","**Покадровое смещение (в пикселях):**\n","$$\n","\\Delta d_t^{(i)} = \\left\\|\\, c_t^{(i)} - c_{t-1}^{(i)} \\,\\right\\|_2,\\quad t=2,\\dots,T_i.\n","$$\n","\n","**Суммарный путь (px):**\n","$$\n","D^{(i)} = \\sum_{t=2}^{T_i} \\Delta d_t^{(i)}.\n","$$\n","\n","**Мгновенная скорость (px/s) на кадре \\(t\\):**\n","$$\n","v_t^{(i)} = \\Delta d_t^{(i)} \\cdot f.\n","$$\n","\n","**Средняя скорость (px/s) за трек:**\n","$$\n","\\bar{v}^{(i)} = \\frac{D^{(i)}}{\\,(T_i-1)/f\\,}\n","= f \\cdot \\frac{1}{T_i-1}\\sum_{t=2}^{T_i}\\Delta d_t^{(i)}.\n","$$\n","\n","**Масштаб (опционально).** Если известен коэффициент \\(\\alpha\\) — метров на пиксель:\n","$$\n","v_{t,\\text{м/с}}^{(i)} = \\alpha\\, v_t^{(i)},\n","\\qquad\n","\\bar{v}_{\\text{м/с}}^{(i)} = \\alpha\\, \\bar{v}^{(i)}.\n","$$\n","\n","**Время в зоне.** Пусть \\(Z\\) — зона интереса (прямоугольник/полигон). Тогда\n","$$\n","T_{\\text{in}}^{(i)}\n","= \\frac{1}{f}\\sum_{t=1}^{T_i}\\mathbf{1}\\!\\left\\{\\, c_t^{(i)} \\in Z \\,\\right\\}\n","\\quad \\text{сек}.\n","$$\n","\n","**Что отображать поверх видео (практика):**\n","- контур зоны \\(Z\\);\n","- bbox и центр \\((x_t,y_t)\\) с подписью ID;\n","- мгновенную скорость \\(v_t\\) (px/s или м/с);\n","- итоговую плашку на кадре/в конце ролика: \\(\\bar{v}\\), \\(T_{\\text{in}}\\), общая длина трека.\n"]},{"cell_type":"code","execution_count":19,"id":"92cddadb","metadata":{"id":"92cddadb","outputId":"5da8095e-468b-459d-ef84-126fbfcd6c35","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762949744140,"user_tz":-180,"elapsed":178378,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ID | avg_speed [px/s] | time_in_zone [s] | tracked_time [s]\n"," 1 |          496.6 |            0.00 |           5.47\n","12 |          575.7 |            0.00 |           0.27\n","13 |          142.6 |            0.00 |           0.23\n","21 |          627.3 |            0.00 |           0.10\n","25 |          828.9 |            0.00 |           0.80\n","39 |          750.0 |            0.00 |           7.07\n","58 |          708.4 |            0.00 |           0.07\n","59 |          639.5 |            0.00 |           1.13\n","61 |           99.9 |            0.00 |           3.80\n","73 |         1628.1 |            0.00 |           0.63\n","75 |          435.2 |            0.00 |           0.30\n","77 |          127.1 |            0.00 |           0.27\n","79 |         1046.7 |            0.00 |           0.07\n","100 |          269.2 |            0.00 |           0.13\n","\n","Видео с оверлеем сохранено: out/metrics_yolo_bytetrack_overlay_h264.mp4\n"]}],"source":["# Метрики по трекингу: средняя скорость (px/s) и время в зоне (сек)\n","# Используем Ultralytics YOLO + встроенный ByteTrack (stream=True), считаем «на лету».\n","# Визуализируем: зона (ROI), толщенные bbox, подписи ID и instant speed.\n","from ultralytics import YOLO\n","import cv2, numpy as np, subprocess, shlex\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","# ----- входные/выходные пути -----\n","SRC = str(videos[0])                                 # исходный ролик (как и раньше)\n","OUT_MP4 = str(OUT_DIR / \"metrics_yolo_bytetrack_overlay_h264.mp4\")\n","\n","# ----- параметры зоны (ROI) -----\n","# Вариант 1: прямоугольник (x1,y1,x2,y2). Подберите под своё видео.\n","ROI_RECT = (50, 50, 600, 400)\n","# Вариант 2 (опционально): произвольный многоугольник (если нужно)\n","ROI_POLY = None  # например: np.array([(100,100),(600,120),(620,420),(80,400)], np.int32)\n","\n","\n","def in_zone(xy):\n","    \"\"\"Проверка попадания точки в ROI.\"\"\"\n","    x, y = xy\n","    if ROI_POLY is not None:\n","        return cv2.pointPolygonTest(ROI_POLY, (float(x), float(y)), False) >= 0\n","    x1, y1, x2, y2 = ROI_RECT\n","    return (x1 <= x <= x2) and (y1 <= y <= y2)\n","\n","def bbox_center_xyxy(b):\n","    x1,y1,x2,y2 = b\n","    return ((x1+x2)/2.0, (y1+y2)/2.0)\n","\n","def draw_metrics_label(img, text_lines, anchor, scale=1.0, pad=8,\n","                       bg=(0,0,0), fg=(255,255,255), alpha=0.65,\n","                       font=cv2.FONT_HERSHEY_SIMPLEX):\n","    # было: font_scale = 1.2 * 4 * scale; thickness = 2 * 4\n","    font_scale = 1.9 * scale                     # ~0.9 для 1080p, до ~1.3 для 4K\n","    thickness  = max(2, int(2 * scale))          # 2..3 обычно\n","    pad        = max(6, int(6 * scale))          # адаптивные отступы\n","\n","    x, y = anchor\n","    line_h = 0; widths = []; heights = []\n","    for s in text_lines:\n","        (w, h), _ = cv2.getTextSize(s, font, font_scale, thickness)\n","        widths.append(w); heights.append(h); line_h = max(line_h, h)\n","\n","    box_w = max(widths) + 2 * pad\n","    box_h = int(len(text_lines) * (line_h + pad)) + pad\n","\n","    overlay = img.copy()\n","    cv2.rectangle(overlay, (x, y), (x + box_w, y + box_h), bg, -1)\n","    cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, dst=img)\n","\n","    ty = y + pad + line_h\n","    for s in text_lines:\n","        cv2.putText(img, s, (x + pad, ty), font, font_scale, fg, thickness, cv2.LINE_AA)\n","        ty += line_h + pad\n","\n","\n","# ----- инициализация -----\n","yolo = YOLO(\"yolov8n.pt\")\n","\n","cap = cv2.VideoCapture(SRC); assert cap.isOpened(), f\"Не открыть {SRC}\"\n","fps = cap.get(cv2.CAP_PROP_FPS) or 25\n","w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","writer = open_h264_writer(OUT_MP4, w, h, fps, crf=20, preset=\"veryfast\")\n","\n","# копия зоны в удобной форме\n","if ROI_POLY is not None:\n","    roi_poly_int = ROI_POLY.reshape(-1, 1, 2).astype(np.int32)\n","else:\n","    x1,y1,x2,y2 = ROI_RECT\n","\n","# ----- аккумуляторы по ID -----\n","# Для каждого ID храним:\n","#  last_center  : последняя точка центра\n","#  dist_sum     : сумма пиксельных смещений по кадрам\n","#  frames       : сколько кадров видели ID\n","#  frames_in_roi: сколько кадров центр был в зоне\n","stats = {}\n","\n","def update_stats(obj_id, center, inroi, step_dist):\n","    st = stats.setdefault(int(obj_id), dict(\n","        last_center=None, dist_sum=0.0, frames=0, frames_in_roi=0\n","    ))\n","    st[\"frames\"] += 1\n","    if inroi:\n","        st[\"frames_in_roi\"] += 1\n","    if step_dist is not None:\n","        st[\"dist_sum\"] += float(step_dist)\n","    st[\"last_center\"] = center\n","\n","# ----- основной цикл: трекинг + визуализация -----\n","# stream=True — экономит RAM и позволяет обрабатывать кадры по одному\n","for r in yolo.track(source=SRC, tracker=\"bytetrack.yaml\", conf=0.25, iou=0.5,\n","                    stream=True, verbose=False):\n","    frame = r.orig_img.copy()\n","    H, W = frame.shape[:2]\n","    #SCALE = max(W, H) / 1920.0  # ~1.0 для 1080p, ~2.0 для 4K\n","    SCALE = min(max(W, H) / 1920.0, 1.9)   # мягкий апскейл, верхняя граница 1.2\n","\n","\n","    # рисуем ROI\n","    if ROI_POLY is not None:\n","        cv2.polylines(frame, [roi_poly_int], isClosed=True, color=(0,255,255), thickness=3)\n","    else:\n","        cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,255), 3)\n","\n","    # читаем боксы/ID\n","    boxes_xyxy = r.boxes.xyxy.cpu().numpy() if r.boxes.xyxy is not None else np.empty((0,4))\n","    ids        = r.boxes.id.cpu().numpy()   if (hasattr(r.boxes, \"id\") and r.boxes.id is not None) else np.array([])\n","    clses      = r.boxes.cls.cpu().numpy()  if r.boxes.cls is not None else np.array([])\n","    confs      = r.boxes.conf.cpu().numpy() if r.boxes.conf is not None else np.array([])\n","\n","    # обрабатываем только людей (COCO class 0) — можно убрать фильтр при желании\n","    for b, oid, cls, cf in zip(boxes_xyxy, ids, clses, confs):\n","        if int(cls) != 0:\n","            continue\n","\n","        # центр + мгновенная скорость (px/s)\n","        cx, cy = bbox_center_xyxy(b)\n","        has_prev = int(oid) in stats and stats[int(oid)][\"last_center\"] is not None\n","        step_dist = None\n","        if has_prev:\n","            px, py = stats[int(oid)][\"last_center\"]\n","            step_dist = np.hypot(cx - px, cy - py)    # пикс/кадр\n","        inst_speed = (step_dist * fps) if step_dist is not None else 0.0  # px/s\n","\n","        # попадание в зону\n","        inside = in_zone((cx, cy))\n","\n","        # обновляем статистику\n","        update_stats(oid, (cx, cy), inside, step_dist)\n","\n","        # визуализация — более «жирные» bbox и надписи\n","        x1i, y1i, x2i, y2i = map(int, b)\n","        color = (0, 200, 0) if inside else (0, 120, 255)\n","        thick = max(4, int(4 * SCALE))                   # рамка чуть тоньше, но адаптивная\n","        cv2.rectangle(frame, (x1i, y1i), (x2i, y2i), color, thick)\n","\n","        lines = [f\"ID {int(oid)}\",\n","                f\"v={inst_speed:.0f} px/s\",\n","                f\"T_in={stats[int(oid)]['frames_in_roi'] / fps:.1f}s\"]\n","\n","        draw_metrics_label(\n","            frame,\n","            text_lines=lines,\n","            anchor=(x1i, max(0, y1i - int(20 * SCALE))), # плашка над bbox\n","            scale=SCALE * 1.9\n","        )\n","\n","        cv2.circle(frame, (int(cx), int(cy)), max(5, int(5 * SCALE)), (255,255,255), -1)\n","\n","\n","        # точка центра\n","        #$cv2.circle(frame, (int(cx), int(cy)), 5, (255,255,255), -1)\n","        cv2.circle(frame, (int(cx), int(cy)), max(6, int(6 * SCALE)), (255, 255, 255), -1)\n","\n","\n","    writer.stdin.write(frame.tobytes())\n","\n","# закрываем пайп\n","writer.stdin.close(); writer.wait()\n","\n","# ----- сводка по объектам -----\n","summary = []\n","for oid, st in stats.items():\n","    if st[\"frames\"] <= 1:\n","        continue\n","    avg_speed_pxps = (st[\"dist_sum\"] / (st[\"frames\"] - 1)) * fps\n","    time_in_zone_s = st[\"frames_in_roi\"] / fps\n","    total_time_s   = st[\"frames\"] / fps\n","    summary.append((oid, avg_speed_pxps, time_in_zone_s, total_time_s))\n","\n","# сортируем по ID и печатаем\n","summary.sort(key=lambda x: x[0])\n","print(\"ID | avg_speed [px/s] | time_in_zone [s] | tracked_time [s]\")\n","for oid, v, tin, tt in summary:\n","    print(f\"{oid:2d} | {v:14.1f} | {tin:15.2f} | {tt:14.2f}\")\n","\n","print(\"\\nВидео с оверлеем сохранено:\", OUT_MP4)\n"]},{"cell_type":"code","source":["display(display_video_from_path(OUT_MP4))"],"metadata":{"id":"ys98oBperWLg","colab":{"base_uri":"https://localhost:8080/","height":932,"output_embedded_package_id":"1ymjJj-YKuJiNvlw6XO_l_1k542wCBQEO"},"executionInfo":{"status":"ok","timestamp":1762949752841,"user_tz":-180,"elapsed":8712,"user":{"displayName":"Mikhail Puzitskiy","userId":"03381470082687778890"}},"outputId":"66cae076-092c-44ad-a4d7-fab63307d13e"},"id":"ys98oBperWLg","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python (cv_course)","language":"python","name":"cv_course"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}